{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n 0.1232 -0.0254 -0.0345 -0.0751  0.0528  0.0952  0.0551 -0.0612  0.0660 -0.0427\n 0.1223 -0.0272 -0.0351 -0.0742  0.0530  0.0955  0.0543 -0.0609  0.0664 -0.0430\n[torch.FloatTensor of size 2x10]\n\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, width_multiplier: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.width_multiplier = width_multiplier\n",
    "        self.conv1: nn.Conv2d = nn.Conv2d(1, 2 * width_multiplier, 5, padding=2)\n",
    "        self.conv2: nn.Conv2d = nn.Conv2d(2 * width_multiplier, 4 * width_multiplier, 5, padding=2)\n",
    "        self.linear: nn.Linear = nn.Linear(4 * width_multiplier * 28 * 28, 64)\n",
    "        self.output: nn.Linear = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x: autograd.Variable) -> autograd.Variable:\n",
    "        y = functional.relu(self.conv1(x))\n",
    "        y = functional.relu(self.conv2(y))\n",
    "        y = functional.relu(self.linear(y.view(-1, 4*self.width_multiplier*28*28)))\n",
    "        y = self.output(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# Now, we test the network to see if it works. \n",
    "net = SimpleCNN(1)\n",
    "print(net(autograd.Variable(torch.rand((2, 1, 28, 28)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy.testing as testing\n",
    "\n",
    "def fsgm(image_batch: torch.FloatTensor,\n",
    "         label_batch: torch.LongTensor,\n",
    "         model: typing.Callable[[autograd.Variable], autograd.Variable],\n",
    "         objective: typing.Callable[[autograd.Variable, autograd.Variable], autograd.Variable],\n",
    "         eps: float):\n",
    "    \"\"\"Takes a batch of images, and modifies each image using the FGSM attack.\"\"\"\n",
    "    for i in range(image_batch.shape[0]):\n",
    "        x = autograd.Variable(torch.unsqueeze(image_batch[i], 0), requires_grad=True)\n",
    "        label = autograd.Variable(label_batch[i:i+1])\n",
    "        output = model(x)\n",
    "        loss = objective(output, label)\n",
    "        loss.backward()\n",
    "        x.data += eps*torch.sign(x.grad.data)\n",
    "        torch.clamp(x.data, min=0.0, max=1.0, out=x.data)\n",
    "\n",
    "\n",
    "# Now, we test to see there are no obvious errors. \n",
    "def test_fsgm():\n",
    "    net = SimpleCNN(1)\n",
    "    image = torch.zeros((1, 1, 28, 28)) + 0.5\n",
    "    label = torch.LongTensor([2])\n",
    "    perturbed_image = image.clone()\n",
    "    fsgm(perturbed_image, label, net, nn.CrossEntropyLoss(), 0.3)\n",
    "    perturbation = torch.abs(perturbed_image - image)\n",
    "    testing.assert_almost_equal(\n",
    "            perturbation.numpy(),\n",
    "            ((perturbation > 0).float()*0.3).numpy(),\n",
    "            15)\n",
    "\n",
    "for i in range(10):\n",
    "    test_fsgm()\n",
    "\n",
    "# What did we learn from this test? How to handle possible zero-gradients. Also, when doing random\n",
    "# initializations, test a few times to make sure that nothing can go wrong with tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(image_batch: torch.FloatTensor,\n",
    "        label_batch: torch.LongTensor,\n",
    "        model: typing.Callable[[autograd.Variable], autograd.Variable],\n",
    "        objective: typing.Callable[[autograd.Variable, autograd.Variable], autograd.Variable],\n",
    "        eps: float,\n",
    "        alpha: float,\n",
    "        num_steps: int,\n",
    "        num_restarts: int):\n",
    "    \"\"\"Runs PGD on the negative of the given loss function with the given parameters on the given image.\"\"\"\n",
    "    \n",
    "    def pgd_without_restarts(sample_index: int):\n",
    "        \"\"\"PGD on negative of the loss function. This has no random restarts.\"\"\"\n",
    "        image = image_batch[sample_index]\n",
    "        x_min = torch.clamp(image - eps, min=0.0)\n",
    "        x_max = torch.clamp(image + eps, max=1.0)\n",
    "        random_start = torch.clamp(image + torch.rand(image.shape)*eps, min=0.0, max=1.0)\n",
    "        x = autograd.Variable(torch.unsqueeze(random_start, 0), requires_grad=True)\n",
    "        for i in range(num_steps):\n",
    "            output = model(x)\n",
    "            label = autograd.Variable(label_batch[sample_index:sample_index + 1])\n",
    "            loss = objective(output, label)\n",
    "            loss.backward()\n",
    "            x.data += alpha*torch.sign(x.grad.data)\n",
    "            x.data = torch.min(torch.max(x.data, x_min), x_max)\n",
    "            x.grad.data.fill_(0)\n",
    "        return x.data, loss.data[0]\n",
    "    \n",
    "    max_loss = -float(\"inf\")\n",
    "    best_perturbed_image = None\n",
    "    for i in range(image_batch.shape[0]):\n",
    "        for _ in range(num_restarts):\n",
    "            perturbed_image, loss = pgd_without_restarts(i)\n",
    "            if loss > max_loss:\n",
    "                max_loss = loss\n",
    "                best_perturbed_image = perturbed_image\n",
    "        image_batch[i] = best_perturbed_image\n",
    "\n",
    "def test_no_runtime_errors():\n",
    "    net = SimpleCNN(1)\n",
    "    image = torch.rand((2, 1, 28, 28))\n",
    "    label = torch.LongTensor([2, 2])\n",
    "    net.zero_grad()\n",
    "    pgd(image, label, net, nn.CrossEntropyLoss(), 0.3, 0.6, 4, 2)\n",
    "\n",
    "\n",
    "# A more fine-grained test. We will create a specific linear model and test that the resulting images fall within a\n",
    "# certain range. \n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, label: int):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear: nn.Linear = nn.Linear(1*28*28, 10)\n",
    "        self.linear.weight.data.fill_(0)\n",
    "        self.linear.weight.data[label, :].fill_(1)\n",
    "    \n",
    "    def forward(self, x: autograd.Variable):\n",
    "        y: autograd.Variable = x.view((-1, 1*28*28,))\n",
    "        return y\n",
    "\n",
    "\n",
    "def test_linear_model():\n",
    "    lin = LinearModel(2)\n",
    "    testing.assert_almost_equal(lin.linear.weight.data[0:2, :].numpy(), torch.zeros((2, 28*28)).numpy(), 15)\n",
    "    testing.assert_almost_equal(lin.linear.weight.data[2, :].numpy(), torch.ones((28*28,)).numpy(), 15)\n",
    "    testing.assert_almost_equal(lin.linear.weight.data[3:, :].numpy(), torch.zeros((7, 28*28)).numpy(), 15)\n",
    "\n",
    "\n",
    "def dummy_loss_function(output_batch: autograd.Variable, label_batch: autograd.Variable) -> autograd.Variable:\n",
    "    return -0.5*torch.sum(nn.MSELoss(reduce=False)(output_batch, autograd.Variable(torch.zeros(\n",
    "        output_batch.data.shape))), dim=1)\n",
    "\n",
    "\n",
    "def test_dummy_loss_function():\n",
    "    identity: autograd.Variable = autograd.Variable(torch.eye(2, 10))\n",
    "    expected_result: autograd.Variable = autograd.Variable(-0.5*torch.ones((2,)))\n",
    "    testing.assert_equal(dummy_loss_function(identity, None).data.numpy(), expected_result.data.numpy())\n",
    "\n",
    "\n",
    "def test_single_step_pgd():\n",
    "    image: torch.FloatTensor = torch.zeros(2, 1, 28, 28) + 0.5\n",
    "    labels: torch.LongTensor = torch.LongTensor([2, 2])\n",
    "    perturbed_image: torch.FloatTensor = image.clone()\n",
    "    pgd(perturbed_image, labels, LinearModel(2), dummy_loss_function, 0.3, 1.0, 1, 20)\n",
    "    testing.assert_almost_equal(perturbed_image.numpy(), torch.zeros(image.shape) + 0.2)\n",
    "\n",
    "\n",
    "def test_multistep_pgd():\n",
    "    image: torch.FloatTensor = torch.zeros(2, 1, 28, 28) + 0.5\n",
    "    labels: torch.LongTensor = torch.LongTensor([2, 2])\n",
    "    perturbed_image: torch.FloatTensor = image.clone()\n",
    "    pgd(perturbed_image, labels, LinearModel(2), dummy_loss_function, eps=0.3, alpha=0.01, num_restarts=1, num_steps=10)\n",
    "    \n",
    "    image_min: torch.FloatTensor = torch.clamp(image - 0.3, min=0.0)\n",
    "    \n",
    "    # Calculate image max\n",
    "    image_max: torch.FloatTensor = image + 0.3\n",
    "    for _ in range(10):\n",
    "        image_max = image_max - 0.01*image_max\n",
    "    image_max = torch.max(image_max, image_min)\n",
    "    \n",
    "    testing.assert_array_less(image_min, perturbed_image)\n",
    "    testing.assert_array_less(perturbed_image, image_max)\n",
    "\n",
    "\n",
    "test_no_runtime_errors()\n",
    "test_dummy_loss_function()\n",
    "test_linear_model()\n",
    "test_single_step_pgd()\n",
    "test_multistep_pgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Now we need to import MNIST and transform it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
