{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n-0.0571 -0.1623 -0.0022  0.1721 -0.1188  0.0514  0.0331  0.0368  0.0556  0.0452\n-0.0579 -0.1702  0.0068  0.1977 -0.0826  0.0469  0.0274  0.0131  0.0441  0.0451\n[torch.FloatTensor of size 2x10]\n\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, width_multiplier: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.width_multiplier = width_multiplier\n",
    "        self.conv1: nn.Conv2d = nn.Conv2d(1, 2 * width_multiplier, 5, padding=2)\n",
    "        self.conv2: nn.Conv2d = nn.Conv2d(2 * width_multiplier, 4 * width_multiplier, 5, padding=2)\n",
    "        self.linear: nn.Linear = nn.Linear(4 * width_multiplier * 28 * 28, 64)\n",
    "        self.output: nn.Linear = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x: autograd.Variable) -> autograd.Variable:\n",
    "        y = functional.relu(self.conv1(x))\n",
    "        y = functional.relu(self.conv2(y))\n",
    "        y = functional.relu(self.linear(y.view(-1, 4*self.width_multiplier*28*28)))\n",
    "        y = self.output(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# Now, we test the network to see if it works. \n",
    "net = SimpleCNN(1)\n",
    "print(net(autograd.Variable(torch.rand((2, 1, 28, 28)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n 0.3063  0.3201\n 0.3940  0.7272\n[torch.FloatTensor of size 2x2]\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n 0.6063  0.6201\n 0.0940  0.4272\n[torch.FloatTensor of size 2x2]\n\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "\n",
    "\n",
    "def fsgm(image_batch: torch.FloatTensor,\n",
    "         label_batch: torch.LongTensor,\n",
    "         model: typing.Callable[[autograd.Variable], autograd.Variable],\n",
    "         objective: typing.Callable[[autograd.Variable, autograd.Variable], autograd.Variable],\n",
    "         eps: float):\n",
    "    \"\"\"Takes a batch of images, and modifies each image using the FGSM attack.\"\"\"\n",
    "    for i in range(image_batch.shape[0]):\n",
    "        x = autograd.Variable(torch.unsqueeze(image_batch[i], 0), requires_grad=True)\n",
    "        label = autograd.Variable(label_batch[i:i+1])\n",
    "        output = model(x)\n",
    "        loss = objective(output, label)\n",
    "        loss.backward()\n",
    "        x.data += eps*torch.sign(x.grad.data)\n",
    "        torch.clamp(x.data, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "# Now, we test to see there are no obvious errors. \n",
    "net = SimpleCNN(1)\n",
    "image = torch.rand((1, 1, 28, 28))\n",
    "label = torch.LongTensor([2])\n",
    "print(image[0, 0, 1:3, 1:3])\n",
    "net.zero_grad()\n",
    "fsgm(image, label, net, nn.CrossEntropyLoss(), 0.3)\n",
    "print(image[0, 0, 1:3, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n 0.4000  0.2835\n 0.3640  0.5573\n[torch.FloatTensor of size 2x2]\n\n\n 0.7000  0.0000\n 0.6640  0.2573\n[torch.FloatTensor of size 2x2]\n\n"
     ]
    }
   ],
   "source": [
    "def pgd(image_batch: torch.FloatTensor,\n",
    "        label_batch: torch.LongTensor,\n",
    "        model: typing.Callable[[autograd.Variable], autograd.Variable],\n",
    "        objective: typing.Callable[[autograd.Variable, autograd.Variable], autograd.Variable],\n",
    "        eps: float,\n",
    "        alpha: float,\n",
    "        num_steps: int,\n",
    "        num_restarts: int):\n",
    "    \"\"\"Runs PGD on the negative of the given loss function with the given parameters on the given image.\"\"\"\n",
    "    \n",
    "    def pgd_without_restarts(sample_index: int):\n",
    "        \"\"\"PGD on negative of the loss function. This has no random restarts.\"\"\"\n",
    "        image = image_batch[sample_index]\n",
    "        x_min = torch.clamp(image - eps, min=0.0)\n",
    "        x_max = torch.clamp(image + eps, max=1.0)\n",
    "        random_start = torch.clamp(image + torch.rand(image.shape)*eps, min=0.0, max=1.0)\n",
    "        x = autograd.Variable(torch.unsqueeze(random_start, 0),\n",
    "            requires_grad=True)\n",
    "        for i in range(num_steps):\n",
    "            output = model(x)\n",
    "            label = autograd.Variable(label_batch[sample_index:sample_index + 1])\n",
    "            loss = objective(output, label)\n",
    "            loss.backward()\n",
    "            x.data += alpha*torch.sign(x.grad.data)\n",
    "            x.data = torch.min(torch.max(x.data, x_min), x_max)\n",
    "            x.grad.data.fill_(0)\n",
    "        return x.data, loss.data[0]\n",
    "    \n",
    "    max_loss = -1.0\n",
    "    best_perturbed_image = None\n",
    "    for i in range(image_batch.shape[0]):\n",
    "        perturbed_image, loss = pgd_without_restarts(i)\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            best_perturbed_image = perturbed_image\n",
    "        image_batch[i] = perturbed_image\n",
    "\n",
    "# Now, we test to see there are no obvious errors. \n",
    "net = SimpleCNN(1)\n",
    "image = torch.rand((1, 1, 28, 28))\n",
    "label = torch.LongTensor([2])\n",
    "print(image[0, 0, 1:3, 1:3])\n",
    "net.zero_grad()\n",
    "pgd(image, label, net, nn.CrossEntropyLoss(), 0.3, 0.6, 4, 2)\n",
    "print(image[0, 0, 1:3, 1:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
